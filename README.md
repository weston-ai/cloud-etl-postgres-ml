# ğŸŒ cloud-etl-postgres-ml

This project showcases a **modular, cloud-native ETL and analytics pipeline** using Google Drive, DuckDB, Supabase (PostgreSQL), and Streamlit â€” all orchestrated from Colab and GitHub.

## ğŸ“Œ Project Goals

- Ingest raw CSV data from Google Drive  
- Explore and clean datasets using DuckDB SQL in Google Colab 
- Load cleaned data into a Supabase-hosted PostgreSQL database  
- Visualize insights via an interactive Streamlit dashboard  
- Maintain a portable, lightweight data science stack from anywhere
- Use PyCharm for heavy dev-op scripting to control script modularity and scalability

---

## ğŸ§  System Architecture & Tool Roles

This project emphasizes a **modular, SQL-centric, cloud-native analytics pipeline** â€” blending Python, SQL, and machine learning workflows with a lightweight, mobile-friendly infrastructure.

### ğŸ”§ Programming + Orchestration

- **Python** â€” The central scripting language for data loading, transformation, validation, and automation.  
- **SQLAlchemy** â€” Enables raw SQL execution and manages secure PostgreSQL connections from Python.  
- **psycopg2** â€” High-performance PostgreSQL adapter used by SQLAlchemy under the hood.  
- **pandas** â€” Handles data manipulation, file I/O, and schema validation. Also bridges CSVs and database tables.

### ğŸ“¦ Lightweight SQL Analytics

- **DuckDB** â€” An embedded SQL engine for querying CSVs directly. Perfect for fast prototyping and EDA in Colab.  
- **Google Colab + Jupyter Notebooks** â€” Used for interactive SQL queries, cleaning, transformation, and EDA â€” no local setup needed.

### ğŸ—„ï¸ Cloud Data Infrastructure

- **Supabase (PostgreSQL)** â€” Secure cloud-hosted PostgreSQL backend for storing cleaned, validated production datasets.  
- **Google Drive** â€” Cloud storage for raw and cleaned CSVs during intermediate ETL stages.

### ğŸ§  Machine Learning

- **scikit-learn** â€” For ML pipelines: classification, regression, clustering, and evaluation.  
- **PyTorch** â€” For future deep learning use cases, especially time-series and tabular neural networks.

### ğŸ‘¨â€ğŸ’» Development Workflow

- **VSCode** â€” Lightweight IDE for quick edits, cloud or local.  
- **PyCharm (Linux WSL2)** â€” Heavy-duty modular development in a full IDE with a Unix-native environment.  
- **Git + GitHub** â€” Full version control using `main`, `dev`, and modular feature branches.

---

## ğŸŒ¿ Git Branch Structure

| Branch Name              | Purpose                                                    |
|--------------------------|-------------------------------------------------------------|
| `main`                   | âœ… Production-ready, deployable branch                      |
| `dev`                    | ğŸ§ª General development and integration staging branch        |
| `duckdb-analytics`       | ğŸ” Colab notebooks for DuckDB-based SQL exploratory analysis |
| `etl-drive-to-supabase`  | ğŸ”„ Python scripts for data cleaning + Supabase uploading     |
| `streamlit-dashboard`    | ğŸ“Š Frontend dashboard for analytics and visual exploration   |
| `ml-modeling` *(optional)* | ğŸ¤– ML pipelines based on Supabase data                   |

---

## ğŸ“ Repository Structure

```text
cloud-etl-postgres-ml/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw CSVs from Google Drive
â”‚   â””â”€â”€ cleaned/                # Cleaned datasets for Supabase
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ eda_duckdb_colab.ipynb # DuckDB SQL queries and data exploration
â”‚   â””â”€â”€ model_dev.ipynb        # Optional: ML notebook
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ upload_to_supabase.py  # ETL logic: upload cleaned data
â”‚   â””â”€â”€ supabase_helpers.py    # DB connection and validation tools
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ schema.sql             # Optional: PostgreSQL schema
â”œâ”€â”€ streamlit/
â”‚   â””â”€â”€ app.py                 # Interactive dashboard UI
â”œâ”€â”€ .env.example               # Template for Supabase DB credentials
â”œâ”€â”€ .gitignore                 # Ignore sensitive and generated files
â”œâ”€â”€ requirements.txt           # Python package dependencies
â””â”€â”€ README.md                  # You are here.
```

## How to use this project

### 1: Clone and Set Up
bash

- git clone git@github.com:weston-ai/cloud-etl-postgres-ml
- cd cloud-etl-postgres-ml
- cp .env.example .env   # copies a template .env.example file to .env (avoids committing secrets)
    - E.G. *supabase\_db\_url = postgresql://postgres:[password]@db.hhzuypfvmrisuumznjid.supabase.co:5432/postgres (This is the url for postgres\_ML database in Supabase)*
- pip install -r requirements.txt  # installs Python dependencies listed in requirements.txt

### 2: Explore Raw Data in DuckDB (Colab)
- Open notebooks/eda_duckdb_colab.ipynb
- Mount Google Drive
- Run SQL queries on raw CSVs using DuckDB directly

### 3: Clean and Upload to Supabase
bash
python scripts/upload_to_supabase.py
- connects to supabase via .env
- Loads cleaned CSVs into Supabase PostgreSQL tables

### 4: Launch Dashboard
bash
cd streamlit
streamlit run app.py

## Notes
- Emphasizes clarity, modularity, and portability
- Can be designed to work entirely from a cloud-based stack (Colab, Supabase, Github, Google Drive)
- Supports mobile data workflows -- ideal for working from cabins, cafes, or campsites
- In reality, PyCharm via WSL2 will be helpful for doing the heavy dev-op lifting (i.e. modular function testing and scalability)

## Contact
Made by Chris Weston
- *Data Science* | *PostgreSQL* | *Lightweight ML*
- Github Repo -- https://github.com/weston-ai/cloud-etl-postgres-ml







